# Лаба 03. Создание витрины данных из разных источников: файлы, NoSQL-хранилища, реляционные базы данных

## О витринах

_Витрина данных (англ. Data Mart; другие варианты перевода: хранилище данных специализированное, киоск данных, рынок данных) — срез хранилища данных, представляющий собой массив тематической, узконаправленной информации, ориентированный, например, на пользователей одной рабочей группы или департамента._ (Wikipedia)

Лабораторная предназначена для реализации на Dataframe API.

В случае проблем смотрите [Полезные советы](../FAQ.md), пункт Решение проблем.

## I. С высоты птичьего полета

У вас есть наборы данных в разных источниках:

* Cassandra – информация о клиентах: uid, пол и возраст.
* Elasticsearch – логи посещения интернет-магазина из экосистемы некоего технологического гиганта: страницы товаров, которые открывали посетители магазина, чтобы посмотреть товар или купить его.
* HDFS – информация о посещениях сторонних веб-сайтов пользователями, приобретенная у вендора: uid, набор (url, timestamp). 
* PostgreSQL - информация о тематических категориях веб-сайтов, приобретенная у вендора.

Вам надо агрегировать данные из этих источников в витрину на основе PostgreSQL для отдела маркетинга, чтобы они могли делать свой анализ и таргетировать предложения для клиентов на основе их предпочтений в сети и в магазине.

![Alt text](/images/img3.png?raw=true "Архитектура")

Адреса Cassandra, Elasticsearch, PostgreSQL находятся [здесь](../tech_info.md)

Для сохранения в PostgreSQL используйте персональную базу имя_фамилия, ваш логин имя_фамилия и пароль из личного кабинета.

## II. Описание данных

### a. Информация о клиентах

Информация о клиентах хранится в Cassandra (keyspace `labdata`, table `clients`) в следующем виде:

* `uid` – уникальный идентификатор пользователя, string
* `gender` – пол пользователя, F или M - string
* `age` – возраст пользователя в годах, integer

### b. Логи посещения интернет-магазина

*Elasticsearch index: visits*

Из бэкенда интернет-магазина приходят отфильтрованные и обогащенные сообщения о просмотрах страниц товаров и покупках. Сообщения хранятся в Elasticsearch в формате json в следующем виде:

* `uid` – уникальный идентификатор пользователя, тот же, что и в базе с информацией о клиенте (в Cassandra), либо null, если в базе пользователей нет информации об этих посетителях магазина, string
* `event_type` – buy или view, соответственно покупка или просмотр товара, string
* `category` – категория товаров в магазине, string
* `item_id` – идентификатор товара, состоящий из категории и номера товара в категории, string
* `item_price` – цена товара, integer
* `timestamp` – unix epoch timestamp в миллисекундах

### c. Логи посещения веб-сайтов

`hdfs:///labs/laba03/weblogs.json`

Логи хранятся в формате json на HDFS и имеют следующую структуру:

* `uid` – уникальный идентификатор пользователя, тот же, что и в базе с информацией о клиенте (в Cassandra),
* массив `visits` c некоторым числом пар (timestamp, url), где `timestamp` – unix epoch timestamp в миллисекундах, `url` - строка.

В этом датасете не все записи содержат `uid`. Это означает, что были покупатели, еще пока не идентифицированные и не внесенные в базу данных клиентов. Покупки и просмотры таких покупателей можно игнорировать в этом задании.

### d. Информация о категориях веб-сайтов

Эта информация хранится в базе данных PostgreSQL `labdata` таблице `domain_cats`:

* `domain` (только второго уровня), string
* `category`, string 

Используйте ваш логин (нужно заменить в логине "." на "_") и пароль от ЛК для соединения с PostgreSQL.

## III. Задание 

Ознакомьтесь с памяткой по [PostgreSQL](../PostgreSQL.md).

Используя psql, создайте в вашей базе данных `имя_фамилия` таблицу `clients`  со следующими колонками:

uid, gender, age_cat, shop_cat1, ... , shop_catN, web_cat1, ... , web_catN

где:
* `uid` (primary key) – uid пользователя.
* `gender` – пол пользователя: `M`, `F`.
* `age_cat` – категория возраста, одна из пяти: `18-24`, `25-34`, `35-44`, `45-54`, `>=55`.
* `shop_cat`, `web_cat` – категории товаров и категории веб-сайтов.

Дайте пользователю `labchecker2` привилегию на `SELECT` из этой таблицы.
> При использовании overwrite при записи в таблицу Postgres, нужно давать гранты на чтение таблицы каждый раз после пересоздания таблицы.

Внимание! 

* Категории товаров берутся из логов посещения интернет-магазина. Чтобы сделать из категорий названия колонок, обратите внимание на метод pivot. Названия категорий приводятся к нижнему регистру, пробелы или тире заменяются на подчеркивание, к категории прибавляется приставка `shop_`. Например  `shop_everyday_jewelry`.

* Категории веб-сайтов берутся из датасета категорий вебсайтов, и точно также из них создаются имена колонок. Например: `web_arts_and_entertainment`.
* После вычленения домена из URL нужно удалить из доменов "www."

В колонках категорий товаров должно быть число посещений соответствующих страниц, а в колонках категорий веб-сайтов - число посещений соответствующих веб-сайтов.

Как оформить лабораторную смотрите ниже.

В случае проблем смотрите [Полезные советы](../FAQ.md), пункт Решение проблем.



# Сквозной проект

Начиная с этой лабораторной работы вы будете работать в рамках одного проекта. Фактически, работа проделанная в одной лабе, будет использоваться в последующих.

Вот этапы проекта:

* Лаба 4a - Сохранение логов из Kafka в Spark по расписанию, с фильтрацией датасета по признаку. Описание ниже.
* Лаба 5 - Подготовка матрицы users x items по логам из Kafka для прогнозирования пола и возраста.
* Лаба 6 - Подготовка матрицы признаков по логам, лежащим на HDFS, для модели машинного обучения
* Лаба 7 - Обучение модели, инференс модели в real-time
* Лаба 8 - Мониторинг качества работы модели машинного обучения.

# Lab04a. Сохранение логов из Kafka в Spark по расписанию

Для приложения аналитики онлайн-магазина поступают данные о визитах пользователя на странички товаров (клики) и о покупках.  
Напишите приложение для обработки событий визитов. Оно должно считывать данные из Kafka, применять фильтр и раскидывать данные в две директории, на основании фильтрации.

На верхнем уровне пайплайн будет выглядеть так: Kafka —> Spark  —> HDFS.
![Alt text](/images/img4a.png?raw=true "Архитектура")

Адрес Kafka смотрите [здесь](../tech_info.md)

>Топик Kafka: lab04_input_data

>Схема для сохранения на hdfs: /user/имя.фамилия/visits/view/ и /user/имя.фамилия/visits/buy/

## Задача с высоты птичьего полета

Вам нужно:

1. Считать события из топика Kafka `lab04_input_data`, используя `read.format("kafka")`. Данные в этом топике уже есть. Описание данных ниже.\
Обратите внимание что, т.к. все записи уже присутствуют в Kafka и не приходят во время работы вашего приложения, стриминг здесь использовать нет необходимости. Т.е., к Kafka можно относится как к обычному источнику данных для Spark - как в 3-ей лабораторной.
 
2. Записывать события с простыми посещениями страниц в HDFS по пути `/user/имя.фамилия/visits/view/имя_колонки=дата`, где `дата` - дата в формате `YYYYMMDD`, 
например `p_date=20200501`. Последняя папка называется партицией (по определенной колонке). Партиционировать нужно по полю, отличному от поля date, например, поле p_date. При этом поле date также должно быть в датасете. Иначе поле date не попадет в файлы внутри папки.

3. А события с покупками – в путь `/user/имя.фамилия/visits/buy/имя_колонки=дата`. 

:warning: Внимание! События фильтруются по полю `event_type`. Также, записываемые события должны обогащаться текстовым полем `date`, которое содержит дату в формате `YYYYMMDD`. То есть, если в сообщении есть timestamp, который соответствует, например, дате 20200301, то к сообщению должно добавится поле `date`, со строковым значением 20200301. Пути p_date создаются на каждую дату, которая присутствует в сообщениях. Сообщения идут последовательно по дате. Формат вывода - такой же как и на входе - json в каждой строчке.

:warning: Внимание! Воспользуйтесь partitionBy

:warning: Внимание! Даты должны соответствовать временной зоне **UTC**.

Ознакомьтесь с краткой инструкцией по [утилитам командной строки](../Kafka.md).

## Описание данных

Сообщения в Kafka выглядят таким образом (в колонке value):

```
{
  "event_type": "buy",
  "category": "Entertainment-equipment",
  "item_id": "Entertainment-equipment-2",
  "item_price": 2529,
  "uid": "40b29579-e845-45c0-a34d-03630d296a81",
  "timestamp": 1577865600000
}
```

где:

- `event_type` – может принимать значение "view" – просмотр страницы, и "buy" - факт покупки.
- `category` – категория товара.
- `item_id` – id товара.
- `item_price` – цена товара.
- `uid` – долгосрочный уникальный идентификатор пользователя.
- `timestamp` – временная метка в миллисекундах эпохи.

### Описание выходных данных

Выходные сообщения в HDFS должны выглядеть таким образом (на каждую дату должна быть отдельная директория):

> `/user/имя.фамилия/visits/view/p_date=20202020`

Пример строки, которую проверяет чекер:

>{"event_type":"view","category":"Mobile-phones","item_id":"Mobile-phones-5","item_price":"1029","timestamp":1588107480000,"uid":"1f295269-22ce-418a-8245-269646990dce","date":"20200428"}


## Оформление работы

В вашем репо в подпапке `lab04a/filter` положите sbt-project под названием filter с главным классом filter в файле filter.scala. Файл filter.scala может лежать в корне проекта или в src/main/scala.

Проект должен компилироваться и запускаться следующим образом:

```
cd lab04a/filter
sbt package
spark-submit --conf spark.filter.topic_name=lab04_input_data --conf spark.filter.offset=earliest --conf spark.filter.output_dir_prefix=/user/имя.фамилия/visits --class filter --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5 ./target/scala-2.11/filter_2.11-1.0.jar
```

### Требования к filter.scala

Ваша программа должна получать следующие конфигурационные параметры с помощью `spark.conf`:

* `spark.filter.topic_name` – название топика для чтения
* `spark.filter.offset` – оффсет (число) в нулевой партиции топика, с которого должно происходить чтение. Также принимаются значение "earliest".
* `spark.filter.output_dir_prefix` – путь (полный или относительный), куда будут писаться фильтрованные данные. 

:warning: Когда вы даете на вход относительный путь, он считается относительно вашей домашней директории в HDFS. То есть, spark.filter.output_dir_prefix=visits эквивалентно spark.filter.output_dir_prefix=/user/имя.фамилия/visits

<!--
В вашей программе должен быть настроен таймаут топика 30 секунд, i.e. `option("consumer_timeout_ms", 30000)`, то есть при отсутствие данных в топике в течение этого времени чтение прекращается. Таким образом, ваша программа не будет висеть вечно.
-->

## Проверка

Проверка осуществляется из [Личного кабинета](https://lk-spark-de.newprolab.com/lab/sb1laba03). 

_На первом этапе_ вы должны запустить ваше приложение со следующими параметрами:

* `spark.filter.topic_name=lab04_input_data`
* `spark.filter.offset=earliest`
* `spark.filter.output_dir_prefix=visits`

Дождитесь окончания работы приложения и убедитесь, что нужные данные записаны в HDFS по пути `/user/имя.фамилия/visits`. Затем запустите чекер.

В данном случае чекер выполняет следующие действия:

- выполнит проверку репозитория, в частности, наличие `filter.scala` 
<!-- и настройки таймаута топика в нем. Так как на втором этапе мы запускаем вашу программу сами, то программа, где не настроен таймаут чтения топика повесит чекер. -->
- считывает файлы и события в них в `hdfs:///user/имя.фамилия/visits/view/*` и `hdfs:///user/name.surname/visits/buy/*`. Отдельно для записей типа `buy`, `view` расcчитывается количество событий в них (subtotals) и "чексумма" (checksums) – сумма всех полей `item_price`.
- логи чекера доступны по пути: `/tmp` на ноде, имена файлов начинаются с вашего логина в личный кабинет (ЛК)

_На втором этапе_ чекер запустит ваше приложение с другим значением оффсета и выходного пути и проверит работу. :warning: Внимание! Ваше приложение будет запускаться чекером в локальном режиме (`--master local[1]`) и output_dir_prefix с протоколом file://. Проверьте работу вашего приложения в этих условиях.

### Описание полей чекера:

* `git_correct`- True/False - репозиторий проверен.
* `info_git_errors` - ошибки, связанные с проверкой репозитория.
* `info_kafka_errors` – возможные ошибки Kafka.
* `info_number_sent` – количество записей в топике `lab04_input_data`.
* `info_number_recieved` – количество записей в директории `visits` у вас в HDFS.
* `total_number_correct` – True/False: совпадает ли общее количество записей в выходных файлах с ответом (в случае False остальные поля чекера не рассчитываются).
* `info_checksums` – "чексуммы" данных для просмотров и покупок. Если здесь пусто, значит в visits/buy, visits/view не лежит то, что чекер может прочитать.
* `info_subtotals` – количество записей в данных для просмотров и покупок. Если здесь пусто, значит в visits/buy, visits/view не лежит то, что чекер может прочитать.
* `checksums_correct` – True/False: чексуммы правильные.
* `subtotals_correct` – True/False: кол-ва записей правильные.
* `offset_handling_correct` – правильно ли обрабатывается параметр офсета (второй этап).
* `info_your_data` = {}: другие данные, полученные чекером. Сюда же записываются данные второго этапа, включая ссылки на логи запускаемых чекером задач (на назначенном вам мастере).
* `info_errors` – остальные ошибки.
* `lab_result` – True/False: общий результат.

## Подсказки

- Вспомните из лекций или посмотрите в Kafka формат данных в колонке value датасета, прочитанного из Kafka. Вспомните из лекций по датафреймам, как можно обрабатывать данный формат.
- Для отладки вы также можете воспользоваться этим же датасетом в HDFS по пути: `hdfs:///labs/laba04/visits-g`.
- Удаляйте выходные файлы перед каждым запуском.
- Параметры передаются в приложение Spark с помощью опции `spark-submit --conf`: 
`--conf spark.filter.topic_name=artem_trunov`
> Если вы хотите получить для тестирования последние 1000 записей из кафки, нужно сначала посчитать count в топике Кафки, 
> затем передать starting offset =(count - 1000);

Для интерпретации дат (timestamp) в зоне UTC, после создания Spark сессии, установите этот параметр: 
```spark.conf.set("spark.sql.session.timeZone", "UTC")```

Максимальная дата, которая должна получится в результирующем датасете - 20200429.

Функция Spark date_format ожидает что timestamp на входе в секундах, обратите внимание в каких единицах timestamp во входном датасете.

:warning: Внимание! Никогда не используйте .master("yarn") при создании сессии в коде!

Как получить значение Spark параметра (который передается из командной через --conf параметр=значение):
```
val parameter1: String = spark.sparkContext.getConf.get("параметр1")
```

Для работы с оффсетами в 4а можно использовать:
```
.option("startingOffsets", 
    if(offset.contains("earliest")) 
        offset
    else { 
        "{\"" + topicName + "\":{\"0\":" + offset + "}}"
    }
)
```

Проверьте:
- что пишите куда чекер сказал

Пример правильного ответа чекера:
```
00_git_correct: True
01_info_git_errors:
02_info_kafka_errors:
03_info_number_sent: 182540
04_info_number_recieved: 182540
05_total_number_correct: True
06_info_checksums: {'buy': 164623083, 'view': 296413771}
07_info_subtotals: {'buy': 65042, 'view': 117498}
08_checksums_correct: True
09_subtotals_correct: True
10_offset_handling_correct: True
11_info_your_data: {'sbt package filter log': '/tmp/alexey.lyzhin_sbt_package_filter.log', 'spark submit filter log': '/tmp/alexey.lyzhin_spark_submit_filter.log', 'Given offset': -1000, 'Spark job records recieved': 1000, 'offset checksums': {'buy': 905243, 'view': 1637398}, 'offset subtotals': {'buy': 354, 'view': 646}}
12_info_errors:
13_lab_result: True
info_offset: 911700
status: last check failed
```

[Полезные советы](../FAQ.md)
